{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MPC's Isolated Tracklet File\n",
    "#### Matt Holman, Matt Payne, Paul Blankley, Ryan Janssen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 December 2017\n",
    "\n",
    "This notebook is intended to be a consolidation of the material that is in ITF_mjh_20171201.ipynb and ITF_mjh.ipynb\n",
    "\n",
    "The plan is to:\n",
    "\n",
    "    * Remove unused routines.\n",
    "    * Select a final, working approach.\n",
    "    * Set up training/test routines.\n",
    "    * Set up routines to run on the full ITF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NOVAS package\n",
    "\n",
    "First, get the USNO's python NOVAS package.  We'll need that.\n",
    "\n",
    "http://aa.usno.navy.mil/software/novas/novas_py/novaspy_intro.php\n",
    "\n",
    "Just type \n",
    "\n",
    "pip install novas\n",
    "\n",
    "pip install novas_de405\n",
    "\n",
    "Here's the reference:\n",
    "\n",
    "Barron, E. G., Kaplan, G. H., Bangert, J., Bartlett, J. L., Puatua, W., Harris, W., & Barrett, P. (2011) “Naval Observatory Vector Astrometry Software (NOVAS) Version 3.1, Introducing a Python Edition,” Bull. AAS, 43, 2011.\n",
    "\n",
    "### The kepcart library\n",
    "\n",
    "You will need to make sure you have a copy of the kepcart library.  There is a copy of it on the MPC bitbucket site, with some instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import math\n",
    "import kepcart as kc\n",
    "import healpy as hp\n",
    "import collections\n",
    "import astropy\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MPC_library\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "from operator import add\n",
    "\n",
    "Observatories = MPC_library.Observatories\n",
    "\n",
    "ObservatoryXYZ = Observatories.ObservatoryXYZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading the MPC observation files\n",
    "\n",
    "Dealing with files line by line in python is not fast.  \n",
    "\n",
    "The itf.txt, NumObs.txt, and UnnObs.txt files have a mix of 1-line and 2-line formats.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This routine checks the 80-character input line to see if it contains a special character (S, R, or V) that indicates a 2-line \n",
    "# record.\n",
    "def is_two_line(line):\n",
    "    note2 = line[14]\n",
    "    return note2=='S' or note2=='R' or note2=='V'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This routine opens and reads filename, separating the records into those in the 1-line and 2-line formats.\n",
    "# The 2-line format lines are merged into single 160-character records for processing line-by-line.\n",
    "def split_MPC_file(filename):\n",
    "    filename_1_line = filename.rstrip('.txt')+\"_1_line.txt\"\n",
    "    filename_2_line = filename.rstrip('.txt')+\"_2_line.txt\"\n",
    "    with open(filename_1_line, 'w') as f1_out, open(filename_2_line, 'w') as f2_out:\n",
    "        line1=None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                if is_two_line(line):\n",
    "                    line1=line\n",
    "                    continue\n",
    "                if line1 != None:\n",
    "                    merged_lines = line1.rstrip('\\n') + line\n",
    "                    f2_out.write(merged_lines)\n",
    "                    line1 = None\n",
    "                else:\n",
    "                    f1_out.write(line)\n",
    "                    line1 = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the files for the three main MPC observation files.  (Need to convert the section below to code from markdown, if you haven't run it already.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed the 2-line observations, for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "split_MPC_file('itf_new.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are routines that read the files after they have been split into their respective formats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMPC_1_line(filename='NumObs_1_line.txt', nrows=1000000):\n",
    "    colspecs = [(0, 5), (5, 12), (12, 13), (13, 14), (14, 15), (15, 32), (32, 44), (44, 56), (65, 71), (77, 80)]\n",
    "    colnames = ['objName', 'provDesig', 'disAst', 'note1', 'note2', 'dateObs', 'RA', 'Dec', 'MagFilter', 'obsCode']\n",
    "    df = pd.read_fwf(filename, colspecs=colspecs, names=colnames, header=None, nrows=nrows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertObs80(line):\n",
    "    objName   = line[0:5]\n",
    "    provDesig = line[5:12]\n",
    "    disAst    = line[12:13]\n",
    "    note1     = line[13:14]\n",
    "    note2     = line[14:15]\n",
    "    dateObs   = line[15:32]\n",
    "    RA        = line[32:44]\n",
    "    Dec       = line[44:56]\n",
    "    mag       = line[65:70]\n",
    "    filt      = line[70:71]\n",
    "    obsCode   = line[77:80]\n",
    "    return objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitMagFilter(magFilter):\n",
    "    pieces = magFilter.split()\n",
    "    if len(pieces)==0:\n",
    "        return None, None\n",
    "    elif len(pieces)==1:\n",
    "         return pieces[0], None\n",
    "    else:\n",
    "        return pieces[0], pieces[1]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am generating a version of the MPC data that includes just the few string-like fields at the start of each line, followed by jd_tdb, the unit vector to the observed target, and the heliocentric position vector of the observatory at that time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This is inactivated because the results have already been generated and don't need to be redone.\n",
    "#\n",
    "with open('itf_new_1_line.mpc', 'w') as outfile:\n",
    "    with open('itf_new_1_line.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.6lf %12.7lf %12.7lf\\n\"% \\\n",
    "                  (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh)\n",
    "            outfile.write(outstring)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS0051 had a typo on the magnitude, a missing '.' in the number.  I corrected that manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preserve tracklets across the time and space boundaries.  That means that the division into time slices and spatial regions should be based on the first member of a tracklet only.  That way the full tracklet is accepted or not, rather than pieces occasionally.\n",
    "\n",
    "The routine below does that, returning a dictionary with trackletIDs as keys and a list of the corresponding observation lines as values.  It also returns a dictionary that gives the starting jd_tdb for each tracklet, as well as a list of tracklets in time-sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_tracklets(itf_filename):\n",
    "    tracklets = defaultdict(list)\n",
    "    tracklets_jd_dict = {}\n",
    "    with open(itf_filename) as infile:\n",
    "        for line in infile:\n",
    "            if not line.startswith('#'):\n",
    "                desig = line[0:12]\n",
    "                jd_tdb = float(line[43:57])\n",
    "                if desig not in tracklets_jd_dict:\n",
    "                    # Got a new tracklet\n",
    "                    tracklets_jd_dict[desig] = jd_tdb\n",
    "                tracklets[desig].append(line)\n",
    "    sortedTrackletKeys = sorted(tracklets.keys(), key=lambda k: tracklets_jd_dict[k]) \n",
    "    return tracklets, tracklets_jd_dict, sortedTrackletKeys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather up this supporting information for the ITF first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracklets, tracklets_jd_dict, sortedTracklets = get_sorted_tracklets('data/itf_new_1_line.mpc')\n",
    "\n",
    "for k in sortedTracklets[:10]:\n",
    "    print(k, tracklets_jd_dict[k])    \n",
    "\n",
    "len(sortedTracklets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now gather up the corresponding supporting information for the UnnObs (unnumbered observations) training set that Matt Payne prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UnnObs_tracklets, UnnObs_tracklets_jd_dict, UnnObs_sortedTracklets = get_sorted_tracklets('data/UnnObs_Training_1_line_A.mpc')\n",
    "\n",
    "for k in UnnObs_sortedTracklets[:10]:\n",
    "    print(k, UnnObs_tracklets_jd_dict[k])    \n",
    "\n",
    "len(UnnObs_sortedTracklets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to separate the data into time windows that are centered on each new moon, so here's a function that takes an integer and returns a corresponding julian date of new moon, rounded to midnight UTC.  Note that this function is approximate but good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the jd of new moon, to the nearest half day\n",
    "def lunation_center(n, tref=2457722.0125, p=29.53055):\n",
    "    t = tref + p*n\n",
    "    tp = np.floor(t) + 0.5\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweep through the tracklets once, outputting them into a sequence of overlapping time ranges that can be processed separately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_time_windows(tracklets, sortedTracklets, tracklets_jd_dict, file_stem='data/itf_new_1_line.mpc', n_begin=-825, n_end=14, dt=15.):\n",
    "    t_center = lunation_center(n_begin)\n",
    "    files = {}\n",
    "\n",
    "    header='#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     '\n",
    "\n",
    "    for desig in sortedTracklets:\n",
    "        jd_tdb = tracklets_jd_dict[desig]\n",
    "        while(jd_tdb>t_center+dt):\n",
    "            if n_begin in files:\n",
    "                files[n_begin].close()\n",
    "            n_begin +=1\n",
    "            t_center = lunation_center(n_begin)\n",
    "        for n in range(n_begin, n_end):\n",
    "            if jd_tdb<lunation_center(n)-dt:\n",
    "                break\n",
    "            if n not in files:\n",
    "                outfile = file_stem.rstrip('.mpc')+'_'+str(lunation_center(n))+'_pm'+str(dt)+'.mpc'\n",
    "                files[n] = open(outfile, 'w')\n",
    "                files[n].write(header+'\\n')\n",
    "            for line in tracklets[desig]:\n",
    "                files[n].write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the ITF data and UnnObs training files into overlapping +/- 15 day windows (the default).\n",
    "\n",
    "The +/-45 day range we were exploring earlier is a little too broad for this model, at present.\n",
    "\n",
    "This only needs to be done once."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "separate_time_windows(tracklets, sortedTracklets, tracklets_jd_dict)\n",
    "separate_time_windows(UnnObs_tracklets, UnnObs_sortedTracklets, UnnObs_tracklets_jd_dict, file_stem='data/UnnObs_Training_1_line_A.mpc', dt=15.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "separate_time_windows(UnnObs_tracklets, UnnObs_sortedTracklets, UnnObs_tracklets_jd_dict, file_stem='data/UnnObs_Training_1_line_A.mpc', dt=45.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get ready to transform the data assuming a heliocentric observer.  The point with this is to be able to figure out which tracklets would be observable, so that we can bring those data together for processing.  The function below will get called repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This returns the topocentric distances and new heliocentric\n",
    "# position vectors to the target, given the assumed distance\n",
    "# r and the position vector of the observatory re.\n",
    "def adjust_position(r, rho_target, re):\n",
    "    rho_x, rho_y, rho_z = rho_target\n",
    "    xe, ye, ze = re\n",
    "    Robs = np.sqrt(xe * xe + ye * ye + ze * ze)\n",
    "    cos_phi = -(rho_x * xe + rho_y * ye + rho_z * ze) / Robs\n",
    "    phi = np.arccos(cos_phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "\n",
    "    xx2 = r*r - Robs*sin_phi * Robs*sin_phi\n",
    "    \n",
    "    if xx2 < 0:\n",
    "        None, None\n",
    "\n",
    "    xx = np.sqrt(xx2)\n",
    "    yy = Robs * cos_phi\n",
    "    \n",
    "    rho_p = yy + xx\n",
    "\n",
    "    # This could be done with numpy arrays\n",
    "    x_p = xe + rho_p*rho_x\n",
    "    y_p = ye + rho_p*rho_y\n",
    "    z_p = ze + rho_p*rho_z\n",
    "    \n",
    "    rho_m = yy - xx\n",
    "    \n",
    "    # This could be done with numpy arrays    \n",
    "    x_m = xe + rho_m*rho_x\n",
    "    y_m = ye + rho_m*rho_y\n",
    "    z_m = ze + rho_m*rho_z\n",
    "        \n",
    "    return (rho_p, (x_p, y_p, z_p)), (rho_m, (x_m, y_m, z_m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the heliocentric transformation, but (as noted above) it is just for the purpose of figuring out where the tracklets would appear in the sky from the sun so that we can select chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Does the transformations on the data using the date of the n-th new\n",
    "# moon as the reference time.\n",
    "#\n",
    "# It is reading and processing the entire *.mpc file.\n",
    "#\n",
    "# This does the heliocentric tranformation for the assumed radius function,\n",
    "# r_func.\n",
    "#\n",
    "# It then does light-time correction.\n",
    "#\n",
    "# And it appends a healpix number on each line in order to be able to quickly \n",
    "# select data from a given region of sky.\n",
    "#\n",
    "# This generates a file called *.trans, and it incorporates\n",
    "# the distance assumed in the file name.\n",
    "#\n",
    "def transform_positions(n, r_func, file_stem='data/itf_new_1_line.mpc', dt=45., nside=8):\n",
    "    infilename = file_stem.rstrip('.mpc')+'_'+str(lunation_center(n))+'_pm'+str(dt)+'.mpc'\n",
    "    try:\n",
    "      open(infilename, 'r')\n",
    "    except IOError:\n",
    "      return 0    \n",
    "    t_ref = lunation_center(n)\n",
    "    r_ref = r_func(t_ref)\n",
    "    r_name = \"_r%.1lf\" % (r_ref)\n",
    "    outfilename = file_stem.rstrip('.mpc')+'_'+str(lunation_center(n))+'_pm'+str(dt)+r_name+'.trans'\n",
    "\n",
    "    #rot_mat = MPC_library.rotate_matrix(-MPC_library.Constants.ecl)\n",
    "    \n",
    "    with open(infilename, 'r') as infile, open(outfilename, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            if line.startswith('#'): \n",
    "                header = line.rstrip()\n",
    "                outfile.write(header + '          dt         x_cor       y_cor        z_cor       pix \\n')\n",
    "            else:\n",
    "                lineID = line[:43]\n",
    "\n",
    "                jd_tdb = float(line[43:57])\n",
    "\n",
    "                x_target, y_target, z_target = line[57:97].split()\n",
    "                r_target = np.array([float(x_target), float(y_target), float(z_target)])\n",
    "\n",
    "                x_obs, y_obs, z_obs = line[97:135].split()            \n",
    "                r_obs = np.array([float(x_obs), float(y_obs), float(z_obs)])\n",
    "                \n",
    "                # This should be a function from here\n",
    "                # Adjust positions\n",
    "                dt = 0.0\n",
    "                r_prev = r_func(jd_tdb-dt)\n",
    "                rho_r_p, rho_r_m = adjust_position(r_prev, r_target, r_obs)\n",
    "                dt = rho_r_p[0]/MPC_library.Constants.speed_of_light\n",
    "                \n",
    "                # Do light-time iterations.\n",
    "                # Probably don't need to do this at this point, because it is\n",
    "                # being re-done in a later step.\n",
    "                i=0\n",
    "                while(np.abs(r_func(jd_tdb-dt)-r_prev)>1e-8):\n",
    "                    rho_r_p, rho_r_m = adjust_position(r_prev, r_target, r_obs)\n",
    "                    dt = rho_r_p[0]/MPC_library.Constants.speed_of_light\n",
    "                    r_prev = r_func(jd_tdb-dt)\n",
    "                    i += 1\n",
    "    \n",
    "                # to here\n",
    "                xp, yp, zp = rho_r_p[1]\n",
    "                \n",
    "                # Calculate HEALPix index\n",
    "                pix = hp.vec2pix(nside, xp, yp, zp, nest=True)\n",
    "            \n",
    "                outstring = line.rstrip() + \" %13.6lf %12.7lf %12.7lf %12.7lf %5d\\n\"% \\\n",
    "                      (dt, xp, yp, zp, pix)\n",
    "\n",
    "                outfile.write(outstring)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the transformation for the ITF with a +/- 15 day window.\n",
    "\n",
    "This only needs to be done once (per distance class).  And after this step the *.mpc files are not needed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for n in range(-825,14):\n",
    "    transform_positions(n, lambda t: 2.5, dt=15.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the transformation for the UnnObs file for a +/- 15 day window."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for n in range(-825,14):\n",
    "    transform_positions(n, lambda t: 2.5, file_stem='data/UnnObs_Training_1_line_A.mpc', dt=15.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for n in range(-825,14):\n",
    "    transform_positions(n, lambda t: 2.5, file_stem='data/UnnObs_Training_1_line_A.mpc', dt=45.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check again\n",
    "# This routine returns the 3-D rotation matrix for the \n",
    "# given reference vector.\n",
    "def xyz_to_proj_matrix(r_ref):\n",
    "    x_ref, y_ref, z_ref = r_ref\n",
    "    r = np.sqrt(x_ref*x_ref + y_ref*y_ref + z_ref*z_ref)\n",
    "    lon0 = np.arctan2(y_ref, x_ref)\n",
    "    lat0 = np.arcsin(z_ref/r)\n",
    "    slon0 = np.sin(lon0)\n",
    "    clon0 = np.cos(lon0)\n",
    "    slat0 = np.sin(lat0)\n",
    "    clat0 = np.cos(lat0)\n",
    "\n",
    "    mat = np.array([[-slon0, clon0, 0], \n",
    "                    [-clon0*slat0, -slon0*slat0, clat0], \n",
    "                    [clon0*clat0, slon0*clat0, slat0 ]])\n",
    "    \n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_tracklet(t_ref, g, gdot, v, GM=MPC_library.Constants.GMsun):\n",
    "    # Here's a version that incorporates radial gravitational\n",
    "    # acceleration\n",
    "    \n",
    "    t_emit = [(obs[0]-obs[1]-t_ref) for obs in v]\n",
    "    acc_z = -GM*g*g\n",
    "    fac =[(1.0 + gdot*t + 0.5*g*acc_z*t*t - g*obs[7]) for obs, t in zip(v, t_emit)]\n",
    "                \n",
    "    A = np.vstack([t_emit, np.ones(len(t_emit))]).T \n",
    "    \n",
    "    x = [obs[2]*f + obs[5]*g for obs, f in zip(v, fac)]                 \n",
    "    mx, cx = np.linalg.lstsq(A, x)[0]\n",
    "            \n",
    "    y = [obs[3]*f + obs[6]*g for obs, f in zip(v, fac)]                 \n",
    "    my, cy = np.linalg.lstsq(A, y)[0]\n",
    "    \n",
    "    return (cx, mx, cy, my, t_emit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_tracklet_grav(t_ref, g, gdot, v, GM=MPC_library.Constants.GMsun, eps2=1e-16):\n",
    "    # Here's a more sophisticated version\n",
    "        \n",
    "    t_emit = [(obs[0]-obs[1]-t_ref) for obs in v]\n",
    "        \n",
    "    # We will approximate g_x(t), g_y(t), and g_z(t)\n",
    "    # using a Taylor expansion.  \n",
    "    # The first two terms are zero by design.  \n",
    "    #\n",
    "    # Given alpha, beta, gamma,\n",
    "    # we would have r_0^2 = (alpha^2 + beta^2 + 1)*z_0^2\n",
    "    # r^2 = (alpha^2 + beta^2 + 1)/gamma^2 ~ 1/gamma^2\n",
    "    # g_x(t) ~ -0.5*GM*x_0*t^2/r^3 + 1/6*jerk_x*t*t*t\n",
    "    # g_y(t) ~ -0.5*GM*y_0*t^2/r^3 + 1/6*jerk_y*t*t*t\n",
    "    # g_z(t) ~ -0.5*GM*z_0*t^2/r^3 + 1/6*jerk_z*t*t*t\n",
    "    #\n",
    "    # We do not have alpha and beta initially,\n",
    "    # but we assert gamma.\n",
    "    # \n",
    "    # We set alpha=beta=0 initially, least squares solve\n",
    "    # the tracklets and obtain alpha, alpha-dot, beta, \n",
    "    # and beta-dot.\n",
    "    # \n",
    "    # Then we use those values to estimate g_x,\n",
    "    # g_y, and g_z for the next iteration.\n",
    "    #\n",
    "    # The process converges when alpha, alpha-dot,\n",
    "    # beta, beta-dot do not change significantly.\n",
    "    #\n",
    "    # We could also do this same process with a \n",
    "    # Kepler-stepper or a full n-body integration.\n",
    "    \n",
    "    alpha = beta = 0.0\n",
    "    alpha_dot = beta_dot = 0.0\n",
    "    cx, cy = 1.0, 1.0\n",
    "    mx, my = 0.0, 0.0\n",
    "        \n",
    "    while(((cx-alpha)*(cx-alpha) + (cy-beta)*(cy-beta))>eps2):\n",
    "            \n",
    "        alpha, beta = cx, cy\n",
    "        alpha_dot, beta_dot = mx, my\n",
    "            \n",
    "        r2 = (alpha*alpha + beta*beta + 1.0)/(g*g)\n",
    "        r3 = r2*np.sqrt(r2)\n",
    "        r5 = r2*r3\n",
    "\n",
    "        x0 = alpha/g\n",
    "        y0 = beta/g\n",
    "        z0 = 1.0/g\n",
    "            \n",
    "        vx0 = alpha_dot/g\n",
    "        vy0 = beta_dot/g\n",
    "        vz0 = gdot/g\n",
    "            \n",
    "        # An alternative to the Taylor expansion is to\n",
    "        # to kepler step from\n",
    "        # x0, y0, z0 and vx0, vy0, vz0 at time 0\n",
    "        # to those at the times of each observation \n",
    "        # in the tracklet.  With that there will be no \n",
    "        # issue of convergence.\n",
    "        # Then simply subtract off the linear motion\n",
    "        # to give the gravitational perturbation.\n",
    "            \n",
    "        rrdot = x0*vx0 + y0*vy0 + z0*vz0\n",
    "        \n",
    "        acc_x = -GM*x0/r3\n",
    "        acc_y = -GM*y0/r3\n",
    "        acc_z = -GM*z0/r3\n",
    "            \n",
    "        jerk_x = -GM/r5*(r2*vx0 - 3.0*rrdot*x0)\n",
    "        jerk_y = -GM/r5*(r2*vy0 - 3.0*rrdot*y0)\n",
    "        jerk_z = -GM/r5*(r2*vz0 - 3.0*rrdot*z0)\n",
    "\n",
    "        fac =[(1.0 + gdot*t + 0.5*g*acc_z*t*t + (1./6.0)*g*jerk_z*t*t*t - g*obs[7]) for obs, t in zip(v, t_emit)]\n",
    "                \n",
    "        A = np.vstack([t_emit, np.ones(len(t_emit))]).T \n",
    "\n",
    "        x = [obs[2]*f + obs[5]*g - 0.5*g*acc_x*t*t - (1./6.0)*g*jerk_x*t*t*t for obs, f, t in zip(v, fac, t_emit)]                 \n",
    "        mx, cx = np.linalg.lstsq(A, x)[0]\n",
    "            \n",
    "        y = [obs[3]*f + obs[6]*g - 0.5*g*acc_y*t*t - (1./6.0)*g*jerk_y*t*t*t for obs, f, t in zip(v, fac, t_emit)]                 \n",
    "        my, cy = np.linalg.lstsq(A, y)[0]\n",
    "        \n",
    "        return (cx, mx, cy, my, t_emit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the one to use.  This routine will be used repeatedly.\n",
    "#\n",
    "# Trying a slightly different approach.\n",
    "# The set of lines that are being passed in have\n",
    "# been selected to be in the same region of sky\n",
    "# for an assumed distance.  We are going to re-transform\n",
    "# those assuming a fixed z (or gamma) value with respect\n",
    "# to the sun and the reference direction, rather than a \n",
    "# fixed r, at the reference time\n",
    "# \n",
    "# Rotate observatory positions to projection coordinates, \n",
    "# and recalculate simple z-based light-time correction.\n",
    "#\n",
    "# Rotate the observations to projection coordinates,\n",
    "# but they will be theta_x, theta_y only\n",
    "#\n",
    "# Fit the simple abg model, for fixed gamma and\n",
    "# possibly gamma_dot.\n",
    "#\n",
    "def select_positions_z(t_ref, g, gdot, vec, lines, outfilename, fit_tracket_func=fit_tracklet):\n",
    "    \n",
    "    #GM = MPC_library.Constants.GMsun\n",
    "\n",
    "    # This rotation is taking things from equatorial to ecliptic\n",
    "    rot_mat = MPC_library.rotate_matrix(MPC_library.Constants.ecl)\n",
    "\n",
    "    results_dict = defaultdict(list)\n",
    "    \n",
    "    # vec is the reference direction in equatorial coordinates\n",
    "    # so we rotate to ecliptic, because we want to.\n",
    "    vec = np.array(vec)\n",
    "    ecl_vec = np.dot(rot_mat, vec)\n",
    "    vec = ecl_vec    \n",
    "    vec = vec/np.linalg.norm(vec)\n",
    "    # mat is a rotation matrix that converts from ecliptic\n",
    "    # vectors to the projection coordinate system.\n",
    "    # The projection coordinate system has z outward,\n",
    "    # x parallel to increasing ecliptic longitude, and\n",
    "    # y northward, making a right-handed system.\n",
    "    mat = xyz_to_proj_matrix(vec)\n",
    "    \n",
    "    # Loop over all the lines from a *.trans file.\n",
    "    for line in lines:\n",
    "        if line.startswith('#'): \n",
    "            # Concatenate all header lines?\n",
    "            header = line.rstrip()\n",
    "        else:\n",
    "            lineID = line[:43]\n",
    "            trackletID = line[0:12]\n",
    "                \n",
    "            jd_tdb = float(line[43:57])\n",
    "            dtp = float(line[139:150])\n",
    "            \n",
    "            # Get unit vector to target\n",
    "            x_target, y_target, z_target = line[58:98].split()\n",
    "            r_target = np.array([float(x_target), float(y_target), float(z_target)])\n",
    "            \n",
    "            # Rotate to ecliptic coordinates\n",
    "            r_target_ec = np.dot(rot_mat, r_target)\n",
    "            \n",
    "            # Rotate to projection coordinates\n",
    "            theta_x, theta_y, theta_z = np.dot(mat, r_target_ec)\n",
    "\n",
    "            # Ignore theta_z after this; it should be very nearly 1.\n",
    "            \n",
    "            # Get observatory position, ultimately in projection coordinates.\n",
    "            x_obs, y_obs, z_obs = line[98:138].split()\n",
    "            r_obs = np.array([float(x_obs), float(y_obs), float(z_obs)])\n",
    "            \n",
    "            # Rotate to ecliptic coordinates\n",
    "            r_obs_ec = np.dot(rot_mat, r_obs)\n",
    "            \n",
    "            # Rotate to projection coordinates            \n",
    "            xe, ye, ze = np.dot(mat, r_obs_ec)\n",
    "            \n",
    "            # This is the light travel time\n",
    "            dlt = ze/MPC_library.Constants.speed_of_light\n",
    "                \n",
    "            # Append the resulting data to a dictionary keye do trackletID.\n",
    "            results_dict[trackletID].append((jd_tdb, dlt, theta_x, theta_y, theta_z, xe, ye, ze))\n",
    "\n",
    "    # Now that we have the observations for each tracklet gathered together,\n",
    "    # we iterate through the tracklets, doing a fit for each one.\n",
    "    results = []\n",
    "    for k, v in results_dict.items():  \n",
    "        \n",
    "        cx, mx, cy, my, t0 = fit_tracklet_func(t_ref, g, gdot, v)\n",
    "        outstring = \"%12s %16.9lf %16.9lf %16.9lf %16.9lf %16.9lf\\n\" % (k, cx, mx, cy, my, t0)\n",
    "        \n",
    "        '''\n",
    "            \n",
    "        # Here's a version that incorporates radial gravitational\n",
    "        # acceleration\n",
    "\n",
    "        t_emit = [(obs[0]-obs[1]-t_ref) for obs in v]\n",
    "        acc_z = -GM*g*g\n",
    "        fac =[(1.0 + gdot*t + 0.5*g*acc_z*t*t - g*obs[7]) for obs, t in zip(v, t_emit)]\n",
    "                \n",
    "        A = np.vstack([t_emit, np.ones(len(t_emit))]).T \n",
    "\n",
    "        # Can I put a simple acc_x term in here?\n",
    "        x = [obs[2]*f + obs[5]*g for obs, f in zip(v, fac)]                 \n",
    "        mx, cx = np.linalg.lstsq(A, x)[0]\n",
    "            \n",
    "        y = [obs[3]*f + obs[6]*g for obs, f in zip(v, fac)]                 \n",
    "        my, cy = np.linalg.lstsq(A, y)[0]\n",
    "\n",
    "        outstring = \"%12s %16.9lf %16.9lf %16.9lf %16.9lf %16.9lf\\n\" % (k, cx, mx, cy, my, t_emit[0])\n",
    "        '''\n",
    "        \n",
    "        results.append(outstring)\n",
    "\n",
    "    if len(results)>0:\n",
    "        with open(outfilename, 'w') as outfile:\n",
    "            outstring = '#  g = %lf\\n' % (g)\n",
    "            outfile.write(outstring)\n",
    "            outstring = '#  gdot = %lf\\n' % (gdot)\n",
    "            outfile.write(outstring)        \n",
    "            outstring = '#  vec= %lf, %lf, %lf\\n' % (vec[0], vec[1], vec[2])\n",
    "            outfile.write(outstring)\n",
    "            outstring = '#  desig              alpha         alpha_dot       beta             beta_dot         dt \\n'\n",
    "            outfile.write(outstring)\n",
    "            for outstring in results:\n",
    "                outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I am doing the same thing as the previous routine, but without files.\n",
    "#\n",
    "# It takes a reference time (t_ref), a set of z, zdot pairs (z_zdot_pairs), \n",
    "# a reference direction vector (vec), and a set of observation lines that \n",
    "# have been selected for a region of sky and time slice (lines)\n",
    "#\n",
    "# It returns a dictionary of results that have z, zdot pairs as keys and\n",
    "# sets of fitted tracklets as results.  Each result has the form:\n",
    "# \n",
    "# trackletID alpha alpha_dot beta beta_dot t_emit,\n",
    "# where t_emit is the light time-corrected time relative to the reference\n",
    "# time.  The coordinates are now in tangent plane projection.\n",
    "#\n",
    "def cluster_positions_z(t_ref, g_gdot_pairs, vec, lines, fit_tracklet_func=fit_tracklet):\n",
    "        \n",
    "    #GM = MPC_library.Constants.GMsun\n",
    "\n",
    "    rot_mat = MPC_library.rotate_matrix(MPC_library.Constants.ecl)\n",
    "\n",
    "    results_dict = defaultdict(list)\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    ecl_vec = np.dot(rot_mat, vec)\n",
    "    vec = ecl_vec    \n",
    "    vec = vec/np.linalg.norm(vec)\n",
    "    mat = xyz_to_proj_matrix(vec)\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('#'): \n",
    "            header = line.rstrip()\n",
    "        else:\n",
    "            lineID = line[:43]\n",
    "            trackletID = line[0:12]\n",
    "                \n",
    "            jd_tdb = float(line[43:57])\n",
    "            dtp = float(line[139:150])\n",
    "            \n",
    "            # Get unit vector to target\n",
    "            x_target, y_target, z_target = line[58:98].split()\n",
    "            r_target = np.array([float(x_target), float(y_target), float(z_target)])\n",
    "            \n",
    "            # Rotate to ecliptic coordinates\n",
    "            r_target_ec = np.dot(rot_mat, r_target)\n",
    "            \n",
    "            # Rotate to projection coordinates\n",
    "            theta_x, theta_y, theta_z = np.dot(mat, r_target_ec)\n",
    "\n",
    "            # Ignore theta_z after this; it should be very nearly 1.\n",
    "            \n",
    "            # Get observatory position\n",
    "            x_obs, y_obs, z_obs = line[98:138].split()\n",
    "            r_obs = np.array([float(x_obs), float(y_obs), float(z_obs)])\n",
    "            \n",
    "            # Rotate to ecliptic coordinates\n",
    "            r_obs_ec = np.dot(rot_mat, r_obs)\n",
    "            \n",
    "            # Rotate to projection coordinates            \n",
    "            xe, ye, ze = np.dot(mat, r_obs_ec)\n",
    "            \n",
    "            dlt = ze/MPC_library.Constants.speed_of_light\n",
    "                \n",
    "            results_dict[trackletID].append((jd_tdb, dlt, theta_x, theta_y, theta_z, xe, ye, ze))\n",
    "            \n",
    "    # All the work done above is independent of the z0 and zdot0 values\n",
    "\n",
    "    master_results = {}\n",
    "    for g_gdot in g_gdot_pairs:\n",
    "        g, gdot = g_gdot\n",
    "\n",
    "        results = []\n",
    "        for k, v in results_dict.items():  \n",
    "            \n",
    "            cx, mx, cy, my, t0 = fit_tracklet_func(t_ref, g, gdot, v)\n",
    "            result = (k, cx, mx, cy, my, t0)\n",
    "            results.append(result)\n",
    "\n",
    "        master_results[g_gdot] = results\n",
    "    \n",
    "    return master_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the above process over all the sky regions, for a given time slice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can probably pass in the clustering function, to reduce code duplication\n",
    "def cluster_sky_regions(g_gdot_pairs, pixels, t_ref, infilename, nside=8, angDeg=7.5, cluster_func=cluster_positions_z):\n",
    "    \n",
    "    # This bit from here\n",
    "    hp_dict = defaultdict(list)\n",
    "    with open(infilename) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            pix = int(line.split()[-1])\n",
    "            hp_dict[pix].append(line)\n",
    "    # to here can be a function that\n",
    "    # accepts infilename and returns\n",
    "    # hp_dict\n",
    "\n",
    "    pixel_results = {}\n",
    "    #for i in range(hp.nside2npix(nside)):\n",
    "    for i in pixels:\n",
    "        print(i)\n",
    "        # Probably don't need to repeat the vector neighbor calculation.\n",
    "        # This can just be stored.\n",
    "        vec = hp.pix2vec(nside, i, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        lines = []\n",
    "        for pix in neighbors:\n",
    "            for line in hp_dict[pix]:\n",
    "                lines.append(line)\n",
    "        if len(lines) > 0:\n",
    "            pixel_results[i] = cluster_func(t_ref, g_gdot_pairs, vec, lines)\n",
    "            \n",
    "    return pixel_results\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some training to determine what is a reasonable resolution for z and z-dot (or gamma and gamma-dot).   We also need to train for dt, the scale factor to relate the angular rates to the angles.  I'll first generate a very fine grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def member_counts(k, sep='|', suff='_'):\n",
    "    keys = k.split(sep)\n",
    "    stems = [key.split(suff)[0] for key in keys]\n",
    "    stem_counter = Counter(stems)\n",
    "    return stem_counter\n",
    "\n",
    "def unique_clusters(test_set):\n",
    "    check_dict = {}\n",
    "    errs=[]\n",
    "    for k in test_set:\n",
    "        stem_counter = member_counts(k)\n",
    "        if len(stem_counter)>1:\n",
    "            errs.append(k)\n",
    "        else:\n",
    "            for k, v in stem_counter.items():\n",
    "                if k not in check_dict:\n",
    "                    check_dict[k] = v\n",
    "                elif v > check_dict[k]:\n",
    "                    check_dict[k] = v\n",
    "    return check_dict, errs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do we want to tune over the velocity scaling factor dt and the cluster size d, we also want to get a sense for how finely we need to sample the z, zdot space to have a good detection efficiency.  I am guessing that with a bigger dt and a bigger d we can get away with a courser grid of z, zdot, but a larger d comes with a somewhat larger contamination rate from overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zs = np.arange(2.5, 2.6, 0.5)\n",
    "#zs = np.arange(3.33, 3.5, 0.5)\n",
    "#zs = np.arange(10.0, 10.1, 0.5)\n",
    "zdots = np.arange(-1e-2, 1.1e-2, 2.0e-3)\n",
    "z_zdots = [(x,y) for x in zs for y in zdots]\n",
    "\n",
    "gs = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "gs =[0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# This could accept a cluster_sky regions function, so that there is only one version of this\n",
    "# code.\n",
    "\n",
    "# And it should accept dt and rad ranges.\n",
    "\n",
    "def do_training_run(pixels, infilename, t_ref, \n",
    "                    cluster_sky_function=cluster_sky_regions,\n",
    "                    g_gdots=g_gdots, mincount=3, \n",
    "                    dts=np.arange(5, 30, 5), \n",
    "                    radii=np.arange(0.0001, 0.0100, 0.0001)):\n",
    "    \n",
    "    master = cluster_sky_function(g_gdots, pixels, t_ref, infilename)\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    rates_dict={}\n",
    "    for dt in dts:\n",
    "        for rad in radii:\n",
    "            cluster_counter = Counter()\n",
    "            for pix, d in master.items():\n",
    "                for z_zdot, arrows in d.items():\n",
    "\n",
    "                    # The bit from here\n",
    "                    i = 0\n",
    "                    label_dict={}\n",
    "                    combined=[]\n",
    "                    for k, cx, mx, cy, my, t in arrows:\n",
    "                        label_dict[i] = k\n",
    "                        combined.append([cx, mx*dt, cy, my*dt])\n",
    "                        i +=1\n",
    "                    points=np.array(combined)\n",
    "                    # to here can be a function,\n",
    "                    # that takes arrows and dt and\n",
    "                    # returns label_dict and points array\n",
    "                    \n",
    "                    # The bit from here\n",
    "                    tree = scipy.spatial.cKDTree(points)\n",
    "                    matches = tree.query_ball_tree(tree, rad)\n",
    "                    # to here can be a function, that takes\n",
    "                    # points are rad and returns tree and\n",
    "                    # matches\n",
    "                    \n",
    "                    for j, match in enumerate(matches):\n",
    "                        if len(match)>=mincount:\n",
    "                            cluster_list =[]\n",
    "                            tracklet_params=[]\n",
    "                            for idx in match:\n",
    "                                cluster_list.append(label_dict[idx].strip())\n",
    "                                #tracklet_params.append(combined[idx])\n",
    "                            cluster_key='|'.join(sorted(cluster_list))\n",
    "                            cluster_counter.update({cluster_key: 1})\n",
    "            \n",
    "            # This region from here \n",
    "            errs = 0\n",
    "            for i, k in enumerate(cluster_counter.keys()):\n",
    "                keys = k.split('|')\n",
    "                stems = [key.split('_')[0] for key in keys]\n",
    "                stem_counter = Counter(stems)\n",
    "                if len(stem_counter)>1:\n",
    "                    errs +=1\n",
    "            # to here can be a function that takes a concatenated\n",
    "            # cluster ID and returns the number of errors\n",
    "\n",
    "            rates_dict[dt, rad] = cluster_counter.keys(), errs\n",
    "\n",
    "    for dt in dts:\n",
    "        values = []\n",
    "        for k, v in rates_dict.items():\n",
    "            dtp, d = k\n",
    "            if dtp==dt:\n",
    "                test_set = list(v[0])    \n",
    "                ncs, nes = len(unique_clusters(test_set)[0]), len(unique_clusters(test_set)[1])\n",
    "                values.append((d, ncs, nes, test_set))\n",
    "\n",
    "        values = sorted(values, key=lambda v: v[0])\n",
    "        ds = [v[0] for v in values]\n",
    "        nclusters = [v[1] for v in values]\n",
    "        nerrors = [v[2] for v in values]\n",
    "        #keys = [v[3] for v in values]\n",
    "        results_dict[dt] = ds, nclusters, nerrors\n",
    "     \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to set up five training runs for the gamma=0.4 distance class.  I will use a very fine grid in gamma-dot (1.25e-3/2.5 = 5e-4.\n",
    "\n",
    "We are looking for the parameters that give us the highest completeness for essentially no errors.  I will look for the parameters, among the five training runs, that achieve that.  Then I will test it on other time slices and data sets.\n",
    "\n",
    "Then I will make the grid spacing more coarse, to see if it is possible to achieve the same results, or nearly the same, for less computational cost.\n",
    "\n",
    "Later, I need to look at different distance classes, perhaps gamma=0.2, 0.3, 0.4, 0.5, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "#gs = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "gs = [0.4]\n",
    "#gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "pix_runs = {}\n",
    "nside=8\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + '.pickle'\n",
    "    for pix in range(hp.nside2npix(nside)):\n",
    "        pix_runs[pix] = do_training_run([pix], infilename, lunation_center(n), g_gdots=g_gdots)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(pix_runs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_sky_regions(pixels, infilename='data/itf_new_1_line_2457397.5_pm45.0_r2.5.trans', nside=8, n=-11, angDeg=7.5):\n",
    "    hp_dict = defaultdict(list)\n",
    "    with open(infilename) as file:\n",
    "        i=0\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            pix = int(line.split()[-1])\n",
    "            hp_dict[pix].append(line)\n",
    "            i += 1\n",
    "            \n",
    "    pixel_results = {}\n",
    "    for i in pixels:\n",
    "        # Probably don't need to repeat the vector neighbor calculation.\n",
    "        # This can just be stored.\n",
    "        vec = hp.pix2vec(nside, i, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        lines = []\n",
    "        for pix in neighbors:\n",
    "            for line in hp_dict[pix]:\n",
    "                lines.append(line)\n",
    "\n",
    "    return lines\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accessible_clusters(pixels, infilename='data/UnnObs_Training_1_line_A_2457397.5_pm15.0_r2.5.trans', mincount=3):\n",
    "    true_counts={}\n",
    "    mergedCounter_dict = {}\n",
    "    mergedTime_dict = {}\n",
    "    for pix in pixels:\n",
    "        lines = output_sky_regions([pix], infilename=infilename)\n",
    "        #print(pix, len(lines))\n",
    "        trackletCounter = Counter()\n",
    "        tracklet_time = defaultdict(float)\n",
    "        for line in lines:\n",
    "            trackletID=line.split()[0]\n",
    "            trackletCounter.update({trackletID : 1})\n",
    "            jd_tdb = float(line[43:57])\n",
    "            tracklet_time[trackletID] = jd_tdb\n",
    "\n",
    "        mergedCounter = Counter()\n",
    "        time_dict = defaultdict(list)\n",
    "        for k, v in trackletCounter.items():\n",
    "            mergedCounter.update({k[:-4]:1})\n",
    "            time_dict[k[:-4]].append(tracklet_time[k])\n",
    "        true_counts[pix]=len([k for k, v in mergedCounter.items() if v>=mincount])\n",
    "        mergedCounter_dict[pix]=mergedCounter\n",
    "        mergedTime_dict[pix]=time_dict\n",
    "    \n",
    "    return true_counts, mergedCounter_dict, mergedTime_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_histogram(mergedCounter):\n",
    "    hist_dict=defaultdict(list)\n",
    "    for k, v in mergedCounter.items():\n",
    "        hist_dict[v].append(k)\n",
    "    return hist_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in [-11]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'pickle'\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        pix_runs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [-11]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'trn2_pickle'\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        pix_runs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Should save these results in files\n",
    "true_count_dict, mergedCounter_dict, mergedTime_dict=accessible_clusters(list(pix_runs.keys()), infilename=infilename)\n",
    "true_count=sum(true_count_dict.values())\n",
    "true_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something general with the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in mergedTime_dict[100].items():\n",
    "    if len(v)>2:\n",
    "        print(\"%s %4d %6.2lf\" % (k, len(v), max(v)-min(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_occ=Counter()\n",
    "for pix in range(hp.nside2npix(nside)):\n",
    "    hist=make_histogram(mergedCounter_dict[pix])\n",
    "    for k, v in hist.items():\n",
    "        num_occ.update({k: len(v)})\n",
    "        \n",
    "num_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for dt in np.arange(10, 80, 10):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    \n",
    "    plt.plot(ds, nclusters, label=dt)\n",
    "\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.axhline(true_count, ls='dashed')\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N clusters')\n",
    "plt.text(0.007, 400, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('nclusters_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dt in np.arange(1, 16, 1):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    \n",
    "    plt.plot(ds, nclusters, label=dt)\n",
    "\n",
    "plt.axhline(true_count, ls='dashed')\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N clusters')\n",
    "#plt.text(0.005, 400, r'$\\gamma=0.4$', fontsize=15)\n",
    "#plt.legend()\n",
    "plt.savefig('nclusters_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dt in np.arange(10, 80, 10):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)\n",
    "    '''\n",
    "    if dt==50:\n",
    "        for d, nc, ne in zip(ds, nclusters, nerrors):\n",
    "            print(d, nc, ne)\n",
    "    '''\n",
    "            \n",
    "    \n",
    "    plt.plot(ds, nerrors, label=dt)\n",
    "\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.ylim((0,3000))\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N errors')\n",
    "plt.text(0.0005, 1000, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('nerrors_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dt in np.arange(1, 16, 1):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)\n",
    "    '''\n",
    "    if dt==50:\n",
    "        for d, nc, ne in zip(ds, nclusters, nerrors):\n",
    "            print(d, nc, ne)\n",
    "    '''\n",
    "            \n",
    "    \n",
    "    plt.plot(ds, nerrors, label=dt)\n",
    "\n",
    "plt.ylim((0,200))\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N errors')\n",
    "#plt.text(0.0005, 1000, r'$\\gamma=0.4$', fontsize=15)\n",
    "#plt.legend()\n",
    "plt.savefig('nerrors_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for dt in np.arange(10, 80, 10):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)    \n",
    "    \n",
    "    plt.plot(nerrors/true_count, nclusters/true_count, label=dt)\n",
    "\n",
    "plt.xlim((0.0001, 0.5))\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Error rate')\n",
    "plt.ylabel('Fraction complete')\n",
    "plt.text(0.05, 0.2, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('AUCish_z2.5.pdf')\n",
    "plt.show()\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dt in np.arange(1, 21, 1):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)    \n",
    "    \n",
    "    plt.plot(nerrors/true_count, nclusters/true_count, label=dt)\n",
    "\n",
    "plt.xlim((0, 0.02))\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('Error rate')\n",
    "plt.ylabel('Fraction complete')\n",
    "plt.text(0.05, 0.2, r'$\\gamma=0.4$', fontsize=15)\n",
    "#plt.legend()\n",
    "plt.savefig('AUCish_z2.5.pdf')\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to plot where the errors are occurring, to see if they are happening near quadrature.  It doesn't seem to be the case.\n",
    "\n",
    "Here's a working goal.  Let's try to achieve 90% completeness with minimal errors.  From the runs I have done, dt=20 does achieve 90% completeness, with a 4% error rate.  \n",
    "\n",
    "I can try a few more dt values, around 20.\n",
    "\n",
    "An alternative approach is to aim to be less complete but with a lower error rate.  And then remove those clusters and iterate.  \n",
    "\n",
    "The errors are false positives.  That is, we say they are clusters, but they are at least contaminated.\n",
    "\n",
    "From the plot below it looks like we want dt=10 days, a completeness of 85%, and an error rate of 1%.  We achieve that at d=0.0008.\n",
    "\n",
    "Actually, if we go with dt=50 days, we can achieve 50% completeness with not recorded errors (or an error rate of less than 0.1%.  Why don't we do with that for a first passs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=-23\n",
    "dt=10\n",
    "earth_vec = Observatories.getObservatoryPosition('500', lunation_center(n))\n",
    "# Actually, this should be related to the angle between the direction of\n",
    "# Earth's motion and the direction of the observations.\n",
    "errs=0\n",
    "clusts=0\n",
    "trues=0\n",
    "for pix in list(pix_runs.keys()):\n",
    "    nclusters = pix_runs[pixels[pix]][dt][1][4]\n",
    "    nerrors = pix_runs[pixels[pix]][dt][2][4]\n",
    "\n",
    "    ntrue = true_count_dict[pix]\n",
    "    vec = hp.pix2vec(nside, pix, nest=True)\n",
    "    ang=180./np.pi * np.arccos(np.dot(earth_vec/np.linalg.norm(vec), vec))\n",
    "    if ang<180:\n",
    "        #print(\"%5d %5d %5d %5d %7.2lf\" % (pix, nclusters, nerrors, ntrue, ang))\n",
    "        errs += nerrors\n",
    "        clusts += nclusters\n",
    "        trues += ntrue\n",
    "errs, clusts, trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pix_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are combinations that resulted in now errors.\n",
    "-11, dt=10, d=[5]\n",
    "-11, dt=20, d=[8]\n",
    "-12, dt=10, d=[11]\n",
    "-12, dt=20, d=[17]\n",
    "-13, dt=10, d=[20]\n",
    "-13, dt=20, d=[26]\n",
    "-15, dt=10, d=[6]\n",
    "-15, dt=20, d=[10] not quite 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am timing a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "pix_runs = {}\n",
    "nside=8\n",
    "\n",
    "pixels= range(hp.nside2npix(nside))\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'trn2_pickle'\n",
    "    for pix in range(hp.nside2npix(nside)):\n",
    "        pix_runs[pix] = do_training_run([pix], infilename, lunation_center(n), g_gdots=g_gdots)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(pix_runs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "nside=8\n",
    "n=-11\n",
    "\n",
    "pixels= range(hp.nside2npix(nside))\n",
    "\n",
    "def do_run(g_gdots=g_gdots, pixels=pixels, infilename='data/itf_new_1_line_2457397.5_pm15.0_r2.5.trans', nside=nside, n=n, angDeg=7.5, dt=5, rad=0.0005, mincount=3):\n",
    "\n",
    "    print('tranforming')\n",
    "    t_ref = lunation_center(n)\n",
    "    master=cluster_sky_regions(g_gdots, pixels, t_ref, infilename, nside=nside, angDeg=angDeg)\n",
    "\n",
    "    print('clustering')\n",
    "    #results_dict = {}\n",
    "    # In the process of clustering here, we need to\n",
    "    # keep track of the 'best' or biggest cluster that\n",
    "    # each tracklet is a member of.\n",
    "    #rates_dict={}\n",
    "    cluster_counter = Counter()\n",
    "    for pix, d in master.items():\n",
    "        for g_gdot, arrows in d.items():\n",
    "            i = 0\n",
    "            label_dict={}\n",
    "            combined=[]\n",
    "            for k, cx, mx, cy, my, t in arrows:\n",
    "                label_dict[i] = k\n",
    "                combined.append([cx, mx*dt, cy, my*dt])\n",
    "                i +=1\n",
    "            points=np.array(combined)\n",
    "            tree = scipy.spatial.cKDTree(points)\n",
    "            matches = tree.query_ball_tree(tree, rad)\n",
    "            for j, match in enumerate(matches):\n",
    "                if len(match)>mincount:\n",
    "                    cluster_list =[]\n",
    "                    tracklet_params=[]\n",
    "                    for idx in match:\n",
    "                        cluster_list.append(label_dict[idx].strip())\n",
    "                        tracklet_params.append(combined[idx])\n",
    "                    cluster_key='|'.join(sorted(cluster_list))\n",
    "                    cluster_counter.update({cluster_key: 1})\n",
    "                    \n",
    "    return cluster_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time first_real_run = do_run(infilename='data/UnnObs_Training_1_line_A_2457308.5_pm15.0_r2.5.trans', n=-14, dt=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lunation_center(-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(first_real_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('itf_new_1_line_2457397.5_pm15.0_z2.5.pickle', 'wb') as handle:\n",
    "    pickle.dump(first_real_run, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "with open('itf_new_1_line_2457397.5_pm15.0_z2.5.pickle', 'rb') as handle:\n",
    "    first_real_run_b = pickle.load(handle)\n",
    "\n",
    "print(first_real_run == first_real_run_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k, v in first_real_run[0].items():\n",
    "    print(k, v)\n",
    "    i += 1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_keys = list(first_real_run.keys())\n",
    "first_keys = sorted(first_keys, key=lambda v: len(v.split('|')), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_original_tracklets_dict(filename='data/UnnObs_1_line.txt'):\n",
    "    tracklets = defaultdict(list)\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            if not line.startswith('#'):\n",
    "                desig = line[0:12].strip()\n",
    "                tracklets[desig].append(line)\n",
    "    return tracklets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_tracklets_dict = get_original_tracklets_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UnnObs_tracklets_dict = get_original_tracklets_dict(filename='data/UnnObs_1_line.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to make a function that generates an MPC-format linkage file to be emailed to Gareth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_original_data(cluster_key, tracklets_dict):\n",
    "    lines=[]\n",
    "    trackletIDs = cluster_key.split('|')\n",
    "    for trackletID in trackletIDs:\n",
    "        for line in tracklets_dict[trackletID]:\n",
    "            lines.append(line)\n",
    "    lines = sorted(lines, key=lambda x: x[15:31])\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('obs.obs', 'w') as file:\n",
    "    for cluster_key in first_keys:\n",
    "        file.write(\"%s\\n\" %(cluster_key))\n",
    "        lines=get_original_data(cluster_key, original_tracklets_dict)\n",
    "        for line in lines:\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in first_keys:\n",
    "    lines = get_original_data(key[:-4], UnnObs_tracklets_dict)\n",
    "    print(key, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Should just passing the selection function, to avoid code duplication\n",
    "def generate_sky_region_files(infilename='data/itf_new_1_line_2457397.5_pm15.0_r2.5.trans', nside=8, n=-11, angDeg=5.5, g=0.4, gdot=0.0):\n",
    "    hp_dict = defaultdict(list)\n",
    "    with open(infilename) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            pix = int(line.split()[-1])\n",
    "            hp_dict[pix].append(line)\n",
    "\n",
    "    for i in range(hp.nside2npix(nside)):\n",
    "        vec = hp.pix2vec(nside, i, nest=True)\n",
    "        neighbors = hp.query_disc(32, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        lines = []\n",
    "        for pix in neighbors:\n",
    "            for line in hp_dict[pix]:\n",
    "                lines.append(line)\n",
    "        outfilename = infilename.rstrip('.trans') + '_hp_' + ('%03d' % (i)) + '_g'+ ('%.2lf' % (g))+'_gdot' + ('%+5.1le' % (gdot))\n",
    "        if len(lines) > 0:\n",
    "            select_positions_z(lunation_center(n), g, gdot, vec, lines, outfilename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lunation_center(-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_sky_region_files(infilename='data/UnnObs_Training_1_line_A_2457279.5_pm15.0_r2.5.trans', n=-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sky_region_files_v2(infilename='data/itf_new_1_line_2457397.5_pm15.0_r2.5.trans', nside=8, n=-11, angDeg=5.5, g=0.4, gdot=0.0):\n",
    "    hp_dict = defaultdict(list)\n",
    "    with open(infilename) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            pix = int(line.split()[-1])\n",
    "            hp_dict[pix].append(line)\n",
    "\n",
    "    for i in range(hp.nside2npix(nside)):\n",
    "        vec = hp.pix2vec(nside, i, nest=True)\n",
    "        neighbors = hp.query_disc(32, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        lines = []\n",
    "        for pix in neighbors:\n",
    "            for line in hp_dict[pix]:\n",
    "                lines.append(line)\n",
    "        outfilename = infilename.rstrip('.trans') + '_hp_' + ('%03d' % (i)) + '_z'+ ('%.2lf' % (z0))+'_zdot' + ('%+5.1le_v2' % (zdot0))\n",
    "        if len(lines) > 0:\n",
    "            select_positions_z_v2(lunation_center(n), g, gdot, vec, lines, outfilename)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z0=3.0\n",
    "for n in range(-11, -9):\n",
    "    infilename = 'itf_new_1_line_'+str(lunation_center(n))+'_pm15.0_r2.5.trans'\n",
    "    print(infilename)\n",
    "    generate_sky_region_files(infilename=infilename, n=n, z0=z0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def make_figure(filename):\n",
    "    plt.ioff()\n",
    "    mxs, cxs, mys, cys, dts =[], [], [], [], []\n",
    "    for line in open(filename):\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        desig, cx, mx, cy, my, dt = line.split()\n",
    "        mxs.append(float(mx))\n",
    "        cxs.append(float(cx))\n",
    "        mys.append(float(my))\n",
    "        cys.append(float(cy))\n",
    "        dts.append(float(dt))\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "    #norm = Normalize()\n",
    "    #norm.autoscale(colors)\n",
    "    \n",
    "    colormap = cm.inferno\n",
    "\n",
    "    plt.quiver(cxs, cys, mxs, mys, dts, scale=0.3, width=0.0003)\n",
    "\n",
    "    plt.xlim(-0.2, 0.2)\n",
    "    plt.ylim(-0.2, 0.2)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('beta')\n",
    "    outfile = filename+'.pdf'\n",
    "    plt.savefig(outfile)\n",
    "    plt.close()\n",
    "    plt.ion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_figure_zoom(filename):\n",
    "    plt.ioff()\n",
    "    mxs, cxs, mys, cys, dts =[], [], [], [], []\n",
    "    for line in open(filename):\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        desig, cx, mx, cy, my, dt = line.split()\n",
    "        mxs.append(float(mx))\n",
    "        cxs.append(float(cx))\n",
    "        mys.append(float(my))\n",
    "        cys.append(float(cy))\n",
    "        dts.append(float(dt))\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "    #norm = Normalize()\n",
    "    #norm.autoscale(colors)\n",
    "    \n",
    "    colormap = cm.inferno\n",
    "\n",
    "    plt.quiver(cxs, cys, mxs, mys, dts, scale=0.3, width=0.0003)\n",
    "\n",
    "    plt.xlim(-0.1, 0.1)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('beta')\n",
    "    outfile = filename+'_zm.pdf'\n",
    "    plt.savefig(outfile)\n",
    "    plt.close()\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lunation_center(-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zdot0=+2.5e-3\n",
    "for z0 in [2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5]:\n",
    "    for n in range(-11, -10):\n",
    "        infilename = 'data/UnnObs_Training_1_line_A_'+str(lunation_center(n))+'_pm45.0_r2.5.trans'\n",
    "        print(infilename)\n",
    "        generate_sky_region_files_v2(infilename=infilename, n=n, z0=z0, zdot0=zdot0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=-15\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "for gdot in gdots:\n",
    "    infilename = 'data/UnnObs_Training_1_line_A_'+str(lunation_center(n))+'_pm45.0_r2.5.trans'\n",
    "    print(infilename)\n",
    "    generate_sky_region_files(infilename=infilename, n=n, g=g, gdot=gdot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z0=3.1\n",
    "for zdot0 in [-1.0e-2, -8.0e-3, -6.0e-3, -4.0e-3, -2.0e-3, 0.0e-3, 2.0e-3, 4.0e-3, 6.0e-3, 8.0e-3, 1.0e-2]:\n",
    "    for n in range(-11, -10):\n",
    "        infilename = 'data/UnnObs_Training_1_line_A_'+str(lunation_center(n))+'_pm45.0_r2.5.trans'\n",
    "        print(infilename)\n",
    "        generate_sky_region_files_v2(infilename=infilename, n=n, z0=z0, zdot0=zdot0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z0=3.1\n",
    "for zdot0 in [-1.0e-2, -8.0e-3, -6.0e-3, -4.0e-3, -2.0e-3, 0.0e-3, 2.0e-3, 4.0e-3, 6.0e-3, 8.0e-3, 1.0e-2]:\n",
    "    for n in range(-11, -10):\n",
    "        infilename = 'data/UnnObs_Training_1_line_A_'+str(lunation_center(n))+'_pm45.0_r2.5.trans'\n",
    "        filename = infilename.rstrip('.trans') + '_hp_378' + ('_z%4.2lf_zdot%+3.1le_v2' % (z0, zdot0))\n",
    "        make_figure(filename)\n",
    "        make_figure_zoom(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "n=-15\n",
    "for g_gdot in g_gdots:\n",
    "    g, gdot = g_gdot\n",
    "    infilename = 'data/UnnObs_Training_1_line_A_'+str(lunation_center(n))+'_pm45.0_r2.5.trans'\n",
    "    filename = infilename.rstrip('.trans') + '_hp_000' + ('_g%4.2lf_gdot%+3.1le' % (g, gdot))\n",
    "    make_figure(filename)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.10_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.20_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.30_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.40_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.60_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.70_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.80_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.90_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.10_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.20_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.30_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.40_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.50_zdot+2.5e-03_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.10_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.20_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.30_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.40_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.60_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.70_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.80_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.90_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.10_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.20_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.30_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.40_zdot+2.5e-03_v2')\n",
    "make_figure('data/UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.50_zdot+2.5e-03_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+0.0e+00')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot-5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot-2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot+7.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.50_zdot-7.5e-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+0.0e+00')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot-5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot-2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot+7.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z3.00_zdot-7.5e-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+0.0e+00')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot-5.0e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot-2.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot+7.5e-03')\n",
    "make_figure('UnnObs_Training_1_line_A_2457397.5_pm45.0_r2.5_hp_378_z2.00_zdot-7.5e-03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a game plan:\n",
    "\n",
    "* Verify the dark time numbers --DONE.\n",
    "* Stick with a smallish time window (+/- 15 days) --DONE.\n",
    "* Do heliocentric transformation for r=2.5 AU (rdot=0) --DONE.\n",
    "* Do selection_positions_z for all the time slices.  This will generate a large number of files.\n",
    "* Identify the largest files.  Find a way to not generate empty files.\n",
    "* Run clustering on each file, identifying candidate clusters.\n",
    "* Somehow vet the candidate clusters.  I can think of three ways: (1) use Gareth's code, (2) use Bernstein's code, or (3) fit a simple linear model.   The third would be a very quick way of clustering tracklets within a lunation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2.5 run is just to separate the tracklets into sky regions.  Given those sky regions, I can examine a runs of z and zdot.  It seems that a simple grid of a few values in z (3 or 4) and zdot (7 or 8) might be enough.\n",
    "\n",
    "Perhaps it is possible to do all the clustering for a sky region of a given time slice all in one pass.  It's not necessary to write out the intermediate files.  Given a set of tracklets, transform for a (z, zdot) value and then search for clusters.\n",
    "\n",
    "If a cluster is found, make a key of the tuple of sorted trackletIDs and then add that key to a dictionary or counter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's a new game plan:\n",
    "\n",
    "* Select 5 different disjoint time slices.  These will be +/- 15 days and +/- 45 days.\n",
    "* Run the training on those time slices for all sky regions for both +/- 15 days and +/- 45 days.  The former requires only the gamma-dot parameter to be varied, for a few values of gammma.  The latter probably requires a finer grid in gamma and gamma-dot.  In both cases, the hyperparameters that we are tuning are the cluster radius and the time factor dt.\n",
    "* If I want to do repeated universal kepler steps, I need to make that fast in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
